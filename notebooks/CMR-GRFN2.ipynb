{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with GRFN v2 data accessible via CMR\n",
    "\n",
    "more about the dataset here:\n",
    "https://aria.jpl.nasa.gov/node/97\n",
    "\n",
    "Earthdata Search Link:\n",
    "https://search.earthdata.nasa.gov/search/granules?p=C1379535891-ASF&pg[0][id]=*-137-*v2_0_0*&m=30.5859375!-120.515625!4!1!0!0%2C2&tl=1534106273!4!!&q=sentinel-1_insar&ok=sentinel-1_insar&sb=-123.32%2C42.00%2C-120.13%2C42.35\n",
    "\n",
    "Things to think about:\n",
    "\n",
    "* How to make things easier for users (in particular browsing catalog and getting into lazy xarray structure for analysis)\n",
    "    * integration of CMR, STAC, intake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from io import StringIO\n",
    "import requests\n",
    "from time import sleep\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple AOI (Oregon - Washington border)\n",
    "# http://geojson.io\n",
    "\n",
    "# not sure how to pass geojson for 'intersects search to cmr, just use bounding box for now'\n",
    "# (lower left longitude, lower left latitude, upper right longitude, upper right latitude.)\n",
    "# NOTE: this bbox includes adjacent swath 'frames', so will make it simpler for now by reducing southern limit\n",
    "#bbox = '-123.32,41.55,-120.13,42.35'\n",
    "bbox = '-123.32,42.00,-120.13,42.35'\n",
    "\n",
    "aoi = json.loads('''\n",
    "{\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -123.321533203125,\n",
    "              41.5579215778042\n",
    "            ],\n",
    "            [\n",
    "              -120.13549804687501,\n",
    "              41.5579215778042\n",
    "            ],\n",
    "            [\n",
    "              -120.13549804687501,\n",
    "              42.35042512243457\n",
    "            ],\n",
    "            [\n",
    "              -123.321533203125,\n",
    "              42.35042512243457\n",
    "            ],\n",
    "            [\n",
    "              -123.321533203125,\n",
    "              41.5579215778042\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMR search results should match these earthdata search results:\n",
    "# two frames\n",
    "# https://search.earthdata.nasa.gov/search/granules?p=C1379535891-ASF&pg[0][id]=*-137-*v2_0_0*&m=30.5859375!-120.515625!4!1!0!0%2C2&tl=1534106273!4!!&q=sentinel-1_insar&ok=sentinel-1_insar&sb=-123.32%2C41.55%2C-120.13%2C42.35\n",
    "# single frame:\n",
    "# https://search.earthdata.nasa.gov/search/granules?p=C1379535891-ASF&pg[0][id]=*-137-*v2_0_0*&m=30.5859375!-120.515625!4!1!0!0%2C2&tl=1534106273!4!!&q=sentinel-1_insar&ok=sentinel-1_insar&sb=-123.32%2C42.00%2C-120.13%2C42.35\n",
    "\n",
    "\n",
    "\n",
    "path = 137\n",
    "fmt = 'json'\n",
    "url = f'https://cmr.earthdata.nasa.gov/search/granules.{fmt}'\n",
    "\n",
    "params = {'collection_concept_id' : 'C1379535891-ASF',\n",
    "          # NOTE: not sure how to get wildcards to work...\n",
    "          #'producer_granule_id' : '*-137-*v2_0_0*',# since path and version are encoded in filename?\n",
    "          'producer_granule_id' : '*GUNW*-137-*v2_0_0*',\n",
    "          'options[producer_granule_id][pattern]':'true', # this seems overly complicated...\n",
    "          #'attribute[]' : f'int,PATH_NUMBER,{path}',\n",
    "          'temporal' : '2014-01-01T00:00:00Z', \n",
    "          'bounding_box' : bbox,\n",
    "          'page_size' : 2000,\n",
    "         }\n",
    "\n",
    "r = requests.get(url, params=params, timeout=100)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(json.loads(r.text)['feed']['entry'])\n",
    "n= len(df)\n",
    "print(f'Found {n} interferograms for path {path}')\n",
    "df.sort_values('time_start', ascending=False, inplace=True) #most recent first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gb = df.granule_size.astype('f4').sum()/1e3\n",
    "print(f'Size of Archive [Gb] = {Gb:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collection_concept_id.unique()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access via URL + URS?\n",
    "# in theory this should work for GDAL>2.4\n",
    "#!GDAL_HTTP_COOKIEJAR=.urs_cookies GDAL_HTTP_COOKIEFILE=.urs_cookies gdalinfo /vsicurl/{url}\n",
    "\n",
    "print(df.producer_granule_id.iloc[0])\n",
    "filename = df.producer_granule_id.iloc[0] + '.nc'\n",
    "print(filename)\n",
    "url = df.links.iloc[0][0]['href']\n",
    "print(url)\n",
    "# Ok, what about directly reading a file into memory with fsspec\n",
    "#import fsspec\n",
    "#files = fsspec.open_files(url, mode='r')\n",
    "#with files[0] as f:\n",
    "#    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMR returns URLS requiring URS authentication, but the data is actually on S3, how to we access directly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMR returns URLS that nCan we access these directly via s3?\n",
    "# https://www.asf.alaska.edu/sar_datasets/sentinel-1-interferograms-beta/command-line-tools/gdal/\n",
    "\n",
    "credential_url = 'https://grfn.asf.alaska.edu/door/credentials'\n",
    "response = requests.get(credential_url)\n",
    "response.raise_for_status()\n",
    "credentials = json.loads(response.text)['Credentials']\n",
    "print(credentials)\n",
    "print('Setting up new AWS Session, expires in 1 hour!')\n",
    "session = boto3.session.Session(credentials['AccessKeyId'], \n",
    "                                credentials['SecretAccessKey'], \n",
    "                                credentials['SessionToken'],\n",
    "                                'us-east-1',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client('s3')\n",
    "#BUCKET = 'gsfc-ngap-asf-grfn-private-prod' # Changed feb 13\n",
    "BUCKET = 'grfn-content-prod'\n",
    "#s3.list_buckets() #access denied error\n",
    "# All files:\n",
    "#s3.list_objects_v2(Bucket=BUCKET)\n",
    "# single file:\n",
    "filename = 'S1-GUNW-A-R-137-tops-20181129_20181123-020010-43220N_41518N-PP-e2c7-v2_0_0.nc'\n",
    "KEY = filename\n",
    "s3.list_objects_v2(Bucket=BUCKET, Prefix=KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicer interface compared to boto\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(key=credentials['AccessKeyId'], \n",
    "                       secret=credentials['SecretAccessKey'], \n",
    "                       token=credentials['SessionToken'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow b/c everything is stored in that bucket! (all GRFN v2 scenes!)\n",
    "#fs.ls(BUCKET)\n",
    "\n",
    "# Download a single file to local file system\n",
    "s3Path = f'{BUCKET}/{filename}'\n",
    "fs.get(s3Path, filename) # Download a single file (~60Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_dataset(filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is stored in an HDF5 'group'\n",
    "ds.close() # Can't open HDF5 file twice it seems...\n",
    "ds = xr.open_dataset(localPath, group='/science/grids/data', engine='h5netcdf',\n",
    "                     #chunks=dict(latitude=682, longitude=1386)) #NOTE: determine chunks in advance or autochunk based on underlaying data?\n",
    "                    )\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly opening to in-memory dataset doesn't work, but should soon:\n",
    "# https://github.com/pydata/xarray/pull/2782\n",
    "# This actually seems like a bug with s3fs, creating an issue\n",
    "fs = s3fs.S3FileSystem(key=credentials['AccessKeyId'], \n",
    "                       secret=credentials['SecretAccessKey'], \n",
    "                       token=credentials['SessionToken'])\n",
    "fileObj = fs.open(s3Path)\n",
    "ds = xr.open_dataset(fileObj, group='/science/grids/data', engine='h5netcdf')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've mirrored to dataset to test on google cloud\n",
    "# Copied data from ASF to Google Storage bucket\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #default anonymous access\n",
    "#images = fs.ls('pangeo-data/grfn-v2/137/')\n",
    "fileObj = fs.open(f'pangeo-data/grfn-v2/137/{filename}') #throws lots of warnings first time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fileObj, group='/science/grids/data', engine='h5netcdf',\n",
    "                     chunks=dict(latitude=682, longitude=1386))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we'll want to load a temporal stack as an xarray dataset:\n",
    "# Example, all the products from the last month\n",
    "#ds = xr.open_mfdataset('S1*nc', concat_dim='band', group='/science/grids/data', engine='h5netcdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
